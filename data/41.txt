StreetVizor: Visual Exploration ofHuman-Scale Urban Forms Based on Street ViewsQiaomu Shen, Student Member, IEEE, Wei Zeng, Member, IEEE, Yu Ye,Stefan Mu¨ ller Arisona, Simon Schubiger, Remo Burkhard, and Huamin Qu, Member, IEEEFig. 1. StreetVizor system. (a) Control panel enables multi-scale navigation, ranking exploration, and feature filtering. (b) Side-by-side map views compare the spatial distribution of human-scale urban forms in two areas-of-interest (AOIs). (c) AOI statistic view presents the quantitative measurements, including correlation, histogram, and diversity in the AOIs shown in (b). (d) Street map views present detailed street views along two streets. (e) Street statistic view extends parallel coordinates with street layouts.Abstract— Urban forms at human-scale, i.e., urban environments that individuals can sense (e.g., sight, smell, and touch) in their daily lives, can provide unprecedented insights on a variety of applications, such as urban planning and environment auditing. The analysis of urban forms can help planners develop high-quality urban spaces through evidence-based design. However, such analysis is complex because of the involvement of spatial, multi-scale (i.e., city, region, and street), and multivariate (e.g., greenery and sky ratios) natures of urban forms. In addition, current methods either lack quantitative measurements or are limited to a small area. The primary contribution of this work is the design of StreetVizor, an interactive visual analytics system that helps planners leverage their domain knowledge in exploring human-scale urban forms based on street view images. Our system presents two-stage visual exploration: 1) an AOI Explorer for the visual comparison of spatial distributions and quantitative measurements in two areas-of-interest (AOIs) at city- and region-scales; 2) and a Street Explorer with a novel parallel coordinate plot for the exploration of the fine-grained details of the urban forms at the street-scale. We integrate visualization techniques with machine learning models to facilitate the detection of street view patterns. We illustrate the applicability of our approach with case studies on the real-world datasets of four cities, i.e., Hong Kong, Singapore, Greater London and New York City. Interviews with domain experts demonstrate the effectiveness of our system in facilitating various analytical tasks.Index Terms—Urban forms, human scale, street view, visual analytics	
1 INTRODUCTIONHuman-scale urban form describes fine-scale characteristics of urban environments that can be directly seen, touched, and experienced by a city’s residents in their daily lives [27]. It is typically measured in high-resolution by sight and hearing, i.e., from several to tens of meters.  Compared with a city’s scale, which is usually measured in kilometers, this scale is human-oriented. As humans pay more attention to interactive surroundings [13], understanding human-scale urban forms is essential for urban planners in designing high-quality urban spaces. However, traditional urbanism theories, such as small-• Qiaomu Shen and Huamin Qu are with the Hong Kong University of Science and Technology. E-mail: {qshen, huamin}@ust.hk.• Wei Zeng and Remo Burkhard are with Future Cities Laboratory, ETH Zurich. Wei Zeng is the corresponding author. E-mail: zeng@arch.ethz.ch.• Yu Ye is with Tongji University. E-mail: yye@tongji.edu.cn.• S. Mu¨ller   risona and S. Schubiger are with University of   pplied Sciences and   rts Northwestern Switzerland FHNW.Manuscript received 31 Mar. 2017; accepted 1 Aug. 2017.Date of publication 28 Aug. 2017; date of current version 1 Oct. 2017.For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below.Digital Object Identifier no. 10.1109/TVCG.2017.2744159
scale surveys and mapping, are hard to provide in-depth guidance for effective urban planning and design at this fine scale.  Given the advancement of various sensing technologies, e.g., cam- eras and GPS devices, we can now quantitatively measure human-scale urban forms by analyzing big urban data. In particular, services, such as Google Street View (GSV) [1], provide detailed panoramic views of urban space from different geographic positions These panoramic views can be utilized to measure various features, including greenery coverage and sky visibility, of human-scale urban forms visible to hu- man eyes. Some pioneering studies have shown that neighborhood environment [33], street-level greenery [22], and even street safety [28] can be precisely assessed from these views.  However, GSV image exploration mainly focuses on either a partic- ular feature (e.g., greenery coverage [22]) or a small area (e.g., neigh- borhood [33] and street [22, 28]). This deficiency limits its applicability in urban planning, where planners need to 1) quantitatively measure multivariate features of urban forms, including not only greenery cover- age, but also sky visibility, and vehicle density [26]; 2) systematically explore urban forms in areas-of-interest (AOIs) at multiple scales, i.e., from small (e.g., streets) to mid (e.g., districts) to large scales (e.g., cities) [25]. In addition, direct means for the comparison of urban forms in two AOIs is desirable to allow planners to utilize information for the quick identification and improvement of factors that affect the quality of urban space.
1077-2626 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

  A visual analytics tool is necessary to fulfill these requirements be- cause it can integrate powerful computing capabilities to quantitatively measure multivariate features, with interactive visual interfaces to sys- tematically explore and compare features in AOIs on demand [37]. Developing such a tool requires considerable effort because of the fol- lowing reasons: first, as cities comprise vast amounts of street views, an efficient feature extraction algorithm is required to automatically uncover human-scale urban forms. Second, the development of a tool for the visual comparison of multivariate features in two AOIs requires an effective visual design that tackles the challenges of spatial, multi- variate, and comparative data visualizations.  In this paper, we introduce StreetVizor, a visual analytics system for the exploration of human-scale urban forms based on GSV im- ages. We develop the system in an iterative design process: specific analysis requirements are described by a collaborating urban planner, and the designs are evaluated and refined against requirements. To present information in concisely, StreetVizor combines a set of well- established visualization techniques, including coordinated multiple views (CMVs) and scatterplot matrix, with a new design of parallel coordinates that integrate street layout information. Our system utilizes advanced clustering models to enable the efficient exploration of street view patterns. We apply StreetVizor in real-world datasets containing∼1.7 million of GSV images of four cities: Singapore, Hong Kong, Greater London, and New York City, and demonstrate its effectivenessthrough interviews with domain experts.The main contributions of this work include:• A fully automatic approach measuring human-scale urban forms by applying deep learning techniques on GSV images;• A visual comparison framework for exploring human-scale urban forms on city-, region-, and street-scales;• A novel visual design of parallel coordinates that integrate street layout information;• Interesting insights revealed from case studies and expert inter- views, such as the negative correlation between greenery and building features, and the differences in street views of two cities.2 RELATED  WORKThis section discusses previous studies closely related to our work.2.1 Street View AnalysisGSV system provides high quality and accurate panoramic images of hundreds of cities [1]. In recent years, researchers studying human- scale urban forms have utilized GSV images as a new and convenient data source. For example, researches have shown that the analysis of GSV images can be used to audit neighborhood environments [33], quantify street greenery [22], and predict street safety [28]. Nonetheless, the majority of these studies face scalability issues given their focus on either a particular feature [22] or a small area [22, 28, 33]. Thees issues can be addressed by incorporating deep learning techniques, which can be used to summarize city landscapes [8] and estimate the demographic makeup of a country [12].  In this work, we collect ∼1.7 million GSV images of four rep- resentative mega-cities, and apply a deep learning technique [3] toautomatically extract desired urban forms from the collected images. More importantly, we develop an effective visual analytics tool for urban planners to explore human-scale urban forms.2.2 Urban Data VisualizationVast amounts of urban data, including traffic [10, 41], social me- dia [6,42], environment [9], and simulated urban spaces [39], have been collected in an urban context. Big urban data brings in unprecedented opportunities for evidence-based urban design, and visualization sys- tems can assist domain experts in finding evidence from the data. A systematic overview of visualization systems can be found in [46].  Qu et al. [32] presented a comprehensive visualization system for the analysis of a city’s air pollution that affects the daily lives of residents. Their system integrates parallel coordinates and scatterplots to show
relationships between high-dimensional air pollutants. In addition to air pollution, landmark visibility is related to the daily experience of a city’s residents. Ortner et al. [30] visually compared the effects of candidate buildings on landmark visibility from various viewpoints. In this system, users can select a series of ranking schemes, and can- didate buildings are then automatically sorted. Similar to our present work, Arietta et al. [2] associated visual elements with city attributes, including violent crime rates and housing prices. They developed var- ious prototype visualizations, such as the visual boundary of urban neighborhoods.  Although different data are explored, these visualizations similarly employ CMVs, because urban data typically exhibits both spatial infor- mation and multi-dimensional attributes. Our system also adopts this empirical approach. In addition, to address specific domain problems, we develop effective visualization techniques, including a novel parallel coordinates enhanced with street layouts.2.3 Multivariate Geographical Data VisualizationVisualizing multivariate data is a hot topic in the visualization field. Numerous conventional approaches to this topic have been developed, and can be classified into two groups: 1) employing visualization techniques, such as parallel coordinates plot (PCP), scatterplot matrix, and start coordinates; and 2) projecting data points onto a two- or three-dimensional visual space that can be directly plotted on a screen, such as multidimensional scaling and principal component analysis. All these approaches have pros and cons. For example, although PCP presents all dimensional attributes without information loss, it can easily generate visual clutter with big data and pairwise correlations can only be shown on two nearby coordinates [18]. Many improvements have been developed to address these issues. These improvements include edge bundling to reduce visual clutter [19, 47], and hierarchical data clustering and the navigation of resulting structures [11, 45].  When multivariate data is dependent upon locations, the analytical tasks become more complex because geographical information needs to be revealed. Turkay et al. developed Attribute Signature [38], which employs a geographical map and small multiples of multivariate at- tributes to show geographic variability in attribute statistics. Goodwin et al. [15] further explored multivariate geographical data across scales by adopting new designs to show correlation, scale, and geographi- cal information. The frameworks proposed by both studies can be generalized to explore multivariate geographical data.  In this work, human-scale urban forms to be explored are also multi- variate geographical data: the features are in six dimensions and they are dependent on locations. We leverage the advantages of scatterplot matrix and PCP for different analytical tasks. Specifically, we employ scatterplot matrix for exploring features at city- and region scales given that it can effectively reveal correlations between all pair-wise features. We also arrange the views in a way similar to Attribute Signature [38], i.e., geographical information is presented on maps and multivariate attributes in small multiples. In addition, we develop a novel PCP enhanced with a themeriver plot, which fits better with the analytical task of showing feature variations along street layout at street-scale.2.4 Comparative VisualizationGleicher et al. [14] classified techniques for visual comparison into three categories: 1) Juxtaposition, i.e., presenting objects next to each other. For example, NodeTrix [43] arranges two human brain networks side-by-side. 2) Superposition, i.e., presenting multiple objects on top of one another. Typical examples are time-series line graphs that plot the changes in several variables over time in the same coordinate system. 3) Explicit encoding, i.e., presenting differences or correlations between objects visually. For instance, the bivariate density map is employed in [44] to show the relationship between departure and arrival movements over space. In practice, these techniques are combined to address complex analytical tasks.  Our work adopts juxtaposition that arranges maps of two AOIs/streets side-by-side (Fig. 1(b) & (d)), and superposition to com- pare multivariate features of two AOIs/streets in the same coordinate system (Fig. 1(c) & (e)).
3 BACKGROUND  AND  ANALYTICAL  TASKSIn this section, we introduce our research background and summarize the desired analytical tasks.3.1 Background
defined [27], its discussions in the context of urban planning has a long history that can be traced back to the 1960s. A series of pioneering studies [13, 20] claimed the positive effects of understanding human- scale urban forms in designing high-quality urban space. Visible human- scale urban forms are particularly important as human beings tend to pay most attentions to surroundings that can be directly seen [13].  Over the past 10 months, we closely worked with a senior researcher (SR) in the field of evidence-based urban design − an emerging research topic in urban planning and design. SR pointed out that though urban planners have begun to realize the importance and usefulness of street views in analyzing visible urban forms (e.g., [22, 28, 33]), systematic and efficient methods that can facilitate exploration remain lacking. Hence, SR proposed the development of an efficient visual analytics tool for exploring human-scale urban forms based on GSV images.  To better understand the problem domain, we conducted several rounds of structured interviews with SR. The main analysis criteria are summarized below:• Multivariate Features. As images contain rich information on the urban environment, the first step is to identify the urban forms for analysis. Here, we identify five key features that can reflect the quality and livability of street spaces [13, 20], i.e., greenery, sky, building, road, and vehicle features. Greenery reflects the pleasing greenery view of a street; sky and building are corre- lated with the sense of street closure negatively and positively, respectively; and increments in road and vehicle ratios decrease the willingness of people to walk and street attractiveness.• Street View Crawling. To reveal the surrounding scenes of a street space, street views have to be crawled appropriately: succes- sive images should reflect the continuous change in surrounding scenes. Hence, the distance between two successive views should not exceed a limit that produces discontinuous scenes; meanwhile, it should not be too small, which will cause computing overload. After experimenting with several options, we find 50 meters is a suitable value for the distance between two successive views.• Street View Directions. Although GSV [16] provides 360- degree panorama imagery, only the front and back images in the directions of street headings at sampling locations are required. Side views are not utilized because of the following considera- tions: First, side views mainly present building facades and street sides and thus cannot correctly reflect other key features of street space, e.g., road. Second, side views are partially contained by the front and back images at nearby sampling locations.  To evaluate the effectiveness of our approach, we first experiment with a few representative cities. SR suggested Hong Kong, Singapore, Greater London, and New York City: Hong Kong and Singapore are dense cities with high-rise buildings in Asia. Greater London and New York City are well-planned cities in Europe and the US.3.2 Analytical TasksAfter identifying the analysis criteria, SR further raised a list of ques- tions for our system to address, including: How are the identified features distributed in an AOI? What are the feature differences be- tween two AOIs? What are the exact views that people can see on a street? Are there any representative views?  Based on these questions, we compile a list of analytical tasks:T.1: Efficient Multi-scale Exploration: Human-scale urban forms are associated with street views at different locations that can be organized on city-, region- and street-scales. Planners first need an intuitive overview of the identified feature distributions within a city or a region (T.1.1). Next, planners need to explore
Fig. 2. Overview of StreetVizor workflow. Our system consists of two phases: data modeling and interactive visual exploration.the details of the urban forms, such as the exact street views, at street level (T.1.2). Effective interactions are required to assist users in navigating across different scales.T.2: Quantitative Measurements: SR emphasized the importance of quantitative measurements to evidence-based urban design. Here, given that an AOI/street can contain vast amounts of street views, planners should analyze the statistics of identified features, in- cluding correlations between features, distributions, and standard deviations (T.2.1). Filtering street views against the values of a specific feature is also important (T.2.2).T.3: Effective Ranking and Comparison: To help planners quickly narrow down the exploration scope, features among multiple AOIs/streets should be effectively ranked (T.3.1). Areas/streets with certain features of high values can be easily discovered for further exploration. After planners select two AOIs/streets, they need to compare the differences in spatial distributions (T.3.2) and the quantitative measurements (T.3.3) of the urban forms.4 SYSTEM  FRAMEWORKStreetVizor is a web-based application comprising two major phases, as illustrated in Fig. 2. In the data modeling phase, our system auto- matically collects hundreds of thousands of GSV images at sampling positions in each city generated from OpenStreetMap (OSM) (Sec- tion 5.1). Then, we classify the pixels of the collected images into 12 classes using SegNet, and extract the desired feature metric from the classification results (Section 5.2). Data collection and preprocessing are conducted offline on a high-performance workstation with 12 core3.40 GHz Intel Core i7-6800K CPU and a GeForce GTX 1080 graphics card. Though enabled with GPU acceleration, the computation still takes several to 20 hours to preprocess images from each city. Then, we construct data structures, including an octree and a lookup table, to facilitate visual exploration, such as spatial query and filtering (Sec- tion 5.3). The datasets are stored in a back-end MongoDB server with2.4 GHz Intel Xeon E5-2620 CPU and 64 GB memory.  The interactive visual exploration phase consists of two stages: 1) Our system provides users with a Ranking Explorer that ranks and compares multiple AOIs/streets based on human-scale urban forms. Users can narrow down exploration by selecting two AOIs/streets for detailed comparison. 2) If two AOIs are selected, the system will present an AOI Explorer that compares the differences in human-scale urban forms in two AOIs at city- and region-scales. The AOI Explorer is composed of CMVs, including two juxtaposition map views for spatial exploration and a superposition statistic view for comparing various quantitative measurements, such as feature correlations and diversities. Users can further navigate down to select two streets, and our system

greenery sky building road vehicle othersFig. 3. Illustration of data preprocessing: sampling locations in New York City are generated from OpenStreetMap (left), a street view image is collected from Google Street View (center), and the image pixels are classified into six features using SegNet (right).will provide Street Explorer, which presents the fine details of human- scale urban forms at street-level. In Street Explorer, we present map views that show the geographical information and representative images of two streets. We also develop a novel PCP enhanced with themeriver along street layouts, allowing users to compare multivariate features and reveal feature distributions along the two streets. The visualization modules are implemented in D3.js and Three.js for different rendering requirements, and they are integrated using Vue.js.5 DATA  MODELINGIn this section, we first describe the collection of GSV images and the extraction of human-scale urban forms from the collected images. Furthermore, we present the methods for data querying and filtering.5.1 Data CollectionBased on the configurations defined in Section 3.1, we develop an automatic approach to collect GSV images. We first download the area of a city from OSM [29] and extract the road network from the OSM data. Next, we apply a flood-fill algorithm that recursively goes through the entire road network every 50 meters, starting from a randomly se-lected location. After this operation is completed, a list of sampling locations ({pos}) with geographic information (lat & long) is gener- ated. We then pass each (lat & long) into GSV API, and extract the corresponding street information (SI), including street name (s name)and heading (h). Finally, we download front and back images at each sampling position by passing lat, long & h with the default field of view and pitch values into GSV API. Together with the downloaded image (Img), we model urban forms at human-scale (U Fhs) at each sampling location as:U Fhs  := < pos, SI, Img >	(1)  We have collected ∼147 k, ∼183 k, ∼685 k and ∼637 k images of Hong Kong, Singapore, Greater London and New York City, respec- tively. Fig. 3 (left) presents all sampling locations (colored dots) in New York City generated from OSM, and (center) shows a sample street view image downloaded from GSV.5.2 Feature ExtractionAfter collecting street views, we first classify the image pixels into 12 classes (e.g., sky and building) using SegNet [3], which is a robust pixel-wise semantic labeling tool with a global accuracy of 82.8%. Among the 12 classes, we count the number of pixels for the identi- fied five features, i.e., greenery, sky, building, road, and vehicle, and summarize the remaining pixels as others. We then normalize the feature data because pixel counts (PC) as raw output values are notintuitive. The normalization is straightforward for each feature value (FV ): FVi = PCi/PCImg, where i ∈ {g, s, b, r, v, o} represent the five features and others. Hence, all feature values are in the range of [0, 1].Then, we can replace the image (Img) with a feature metric (FM) as:Img → FM := < FVg, FVs, FVb, FVr, FVv, FVo >	(2)  Fig. 3 (right) shows the classification result of the street view image in the center produced by SegNet.5.3 Data Querying and FilteringBased on Equations 1 and 2, we model human-scale urban forms (U Fhs) with the following attributes: position (pos), street information (SI), and feature metric (FM). By nature, the data exhibits the following
spatial units streets street viewsgreenery	skybuilding	roadvehicle	othersFig. 4. Data model: street views with six-dimensional features of green- ery, sky, building, road, vehicle and others, are organized in an octree structure and a street lookup table.properties: 1) spatial, i.e., positions; 2) multi-scale, because the posi- tions can be hierarchically grouped in accordance with city and regional units, or street information; and 3) multivariate, i.e., the feature metric is in six-dimensional data space.  These complex data natures bring in challenges for our analytical tasks. To address these challenges, we further identify the following querying and filtering models that our system should support:• Spatial Query: To overview the feature distributions within an AOI (T.1.1), our system should first support an efficient query of a list of {U Fhs} with their pos laying in a given AOI. The AOIcan be either an administrative zone (e.g., a city or a district) ora user-specified region defined using a lasso tool.  We achieve the efficient spatial query operation by organizing all {U Fhs} in acity in a four-level octree structure, in which the topmost level is the boundary of each city.• Street Query: To support the exploration of human-scale urban forms at street-scale (T.1.2), our system should allow users to interactively query a street by its name. Here, we create a lookup table with street names as keys, and store corresponding U Fhs ids in each street slot.• Feature Filtering: To accelerate filtering against a particular feature (T.2.2), our system first sorts all {U Fhs} to be explored in increasing order for every feature.  Then, we adopt a binarysearch approach in run time.  Fig. 4 illustrates the data model that organizes the street views in an octree structure and a street lookup table. Each street view contains the six-dimensional features of greenery, sky, building, road, vehicle, and others. We store these querying and filtering models are stored in a back-end MongoDB database, as shown in Fig. 2.6 VISUALIZATION  DESIGNIn this section, we first discuss the rationales behind our visualization design. Then, we provide a detailed description of the visualization techniques implemented in our system.6.1 Design RationalesTo address the complex analytical tasks (Section 3.2), a proper visual design should follow the design rationales below:R.1: Overview+Details: To facilitate multi-scale exploration, our sys- tem should follow the information-seeking mantra: “Overview first, zoom and filter, then details on demand” [36]. First, the system should provide an overview of human-scale urban forms at city- and region-scales, and then allow users to explore more details at street-scale. Efficient query and filtering should be provided to enable smooth transitions between these scales.R.2: Coordinated Multiple Views: Our system should effectively reveal multiple perspectives information of human-scale urban forms, including geographical locations and multivariate features. CMVs that present linked information and allow users to explore data from multiple joint-perspectives fulfill this requirement.R.3: Effective Comparison: To enable effective data comparison, dif- ferent comparative visualization techniques should be employed

for multiple-perspective information. Specifically, given that spatial information for two AOIs/streets is unsuitable for direct overlay, we adopt side-by-side map views. On the other hand, the feature metric can be mapped on the same scope. Therefor, we select a superposition visualization to reveal the differences.R.4: Visual Consistency: Since multi-scale and multiple-perspective visualizations are to be designed, the system should maintain visual consistency across different visualization modules. We realize visual consistency by 1) applying the same layout, i.e., presenting spatial information on the top and quantitative mea- surements on the bottom, in AOI Explorer and Street Explorer;2) employing consistent color mappings. Specifically, we set green, light blue, orange, brown, light pink, and gray to represent the features of greenery, sky, building, road, vehicle, and others, respectively. AOIs/streets on the left and right side are colored as red and blue, respectively.6.2 Ranking ExplorerRanking Explorer is developed to overview feature attributes across multiple AOI/street candidates to help users quickly identify AOIs/streets for comparison (T.3.1). The explorer presents each can- didate as a row and arranges its multivariate information in seven columns. The first column provides general information, such as city and region/street id. The remaining columns present the six features’ mean values as bars, where mean values are normalized and encoded by bar lengths. Clicking the body of a feature column of interest will expand the column as a boxplot to show statistical distributions. The explorer also allows users to sort candidates against a particular feature and the ranking will update correspondingly. Such designs have been well established and evaluated in a previous work [24].  Fig. 1(a) ranks all districts in Hong Kong in accordance with building feature (red dashed box). As an example, we observe the detailed statistics of the sky feature. We select the column for this fea- ture and expand it to boxplot. By comparing the orderings of greenery, sky, and building features, we find greenery and sky features are cor- related, while they are negatively correlated with the building feature. This information helps users narrow down the comparison choices to 1) district HK 16 (Yau Tsim Mong) with the highest building ratio and 2) district HK 7 (Sai Kung) with low building and high greenery ratios, as shown in Figs. 1(b) & (c).6.3 AOI ExplorerAOI Explorer is developed to provide efficient comparison of human- scale urban forms at city- and region-scales (T.1.1). The explorer integrates coordinated multiple views, including:6.3.1 AOI Map ViewWe develop side-by-side map views (Figure. 1(b)) to compare the spatial distributions of human-scale urban forms in two AOIs (T.3.2). Each map view consists of: 1) a background map layer implemented with Leaflet.js to allow users to change map style (e.g. satellite, street, and sport) for different purposes; and 2) a point density map overlaid on top of the background map layer, with points representing street views. The density points are evenly sampled on each street with an upper limit of 10,000 points. Point color corresponds to the maximum feature value in the street view image. A corresponding street view image will pop up when the mouse pointer hovers over a point. Users can select two cities or regions from the navigation panel, or directly manipulate AOIs on the map views with a lasso tool.  Heat map is an alternative design for the point density map. However, in this work, we focus on the simultaneous analysis of multiple features of human-scale urban forms. Compositing these features into one heat map [34] will require redundant user interactions. In addition, sampling positions are generated along the street network. Thus, no street views is collected from many places across a city. In this case, the heat map will generate ambiguity between the two scenarios of 1) no record or 2) low feature values in a region.
0.5ghFig. 5. The AOI Statistic View combines (a) a scatterplot matrix to show pair-wise correlations between two features, (b) small multiples of histogram bar charts to overview feature distributions, and (c) small multiples of deviation plots to present feature diversity.6.3.2 AOI Statistic ViewBesides the map view, which enables the comparison of spatial distri- butions of human-scale urban forms, we develop AOI Statistic View to allow users to compare various quantitative measurements (T.3.3). Nonetheless, each AOI may contain too many street views (up to several hundred thousand) that will overload the rendering process. Moreover, the street views will occlude each other if we simply plot all of them. To address this problem, we cluster street views into groups based on either their administrative units (districts, divisions, and streets) or a mean-shift clustering algorithm [7] that works as follows.Mean Shift Clustering. We cluster urban forms based on their fea- ture metric and geographical positions. Our algorithm works in the following way:  given an input list of N human-scale urban forms{U Fhs}, we first normalize the lat & long attributes of each U Fhsagainst the boundary of all {U Fhs}.  Then, we combine the normal-ized latnor & longnor with the feature metric to construct a new dataset X := {x1, x2, ..., xN}, where each data point xi is in an eight-dimensional space, R8 := < latnor , longnor, FVg, FVs, FVb, FVr, FVv, FVo >. The distance between two data points is measured as their Euclidean dis-tance. We then estimate a bandwidth h from X , and apply mean-shift clustering algorithm on X using a flat kernel. Here, the bandwidth estimation and mean shift clustering are performed with a machine learning library scikit-learn [31]. Finally, we generate a list of m urbanform clusters C := {c1, c2, ..., cm}, where each cluster ci contains n data point ci := {x1, x2, ..., xn}.  The clustering process groups geographically close street views withsimilar feature attributes together. Given that locations are integrated as two-dimensional spaces, the algorithm forms more local clusters than an algorithm without considering spatial information, which usually generates several big clusters with too many street views. The algo- rithm may be further improved by adopting a network-based distance measurement approach other than Euclidean distance. We expect to form more representative clusters of street views with street network information. Nonetheless, the current approach fulfills our requirement of reducing visual clutter.  After generating the clusters C, we further compute and visualize the following quantitative measurements:• Feature Correlation: We first compute the mean values of the identified features, i.e., greenery, sky, building, road, vehicle, and others in each cluster ci. We then plot the pair-wise correlations using a scatterplot matrix (Fig. 5(a)). Notice that the clusters in the left and right AOIs are colored red and blue, respectively.• Feature Histogram: Though feature values fall in the range of [0% - 100%], we seldom find feature values that exceed 50%. Thus, we only consider the range [0% - 50%]. For each feature, we divide the range into 10 even parts, i.e., [0% - 5%), [5% - 10%)..., and aggregate the corresponding value in each street view to each part. Then, histograms for each feature are plotted as bar charts in an up and down manner for AOIs on the left and right, respectively. The six histogram bar charts are arranged in small multiples next to the scatterplot matrix (Fig. 5(b)).
abounding box
rotationb	cB	B

primary axisA
rendering
Fig. 6. Street Map View provides an overview of all street views along a street as colored points, and highlights images on two sides of the street.(a) A tree map showing the feature composition of an image will pop up when the mouse pointer is hovered over the image.
mored  greenery
A  space
• Feature Diversity: We use standard deviation to indicate the measured feature diversity measured for each cluster, and design diversity views that are arranged as six side-by-side small mul- tiples for each feature as shown in Figure 5(c). In every feature diversity view, each cluster is represented as a dot with its x-value indicating the averaged feature value and y-value indicating stan- dard deviation. In addition, we use line segments to indicate the largest and smallest feature values in each cluster. In case an AOI may have hundreds of clusters that lead to serious visual clutter problem, we implement a contour map view [5] (Fig. 5(h)) to show overall distribution patterns. Users can interactively choose one of the views in accordance with different requirements.  The AOI Statistic View comparison of New York City (red) and Hong Kong (blue) is shown in Fig. 5. From the view, we can easily identify some obvious patterns. For example, in both cities, greenery and building features are negatively correlated (Fig. 5(d)). The correlations between other pairwise features, e.g., greenery and road (Fig. 5(e)), are not obvious. From Fig. 5(f), we find that the vehicle ratio is higher in New York City than in Hong Kong. In addition, we find that although the greenery values are similar for both cities, diversity is higher in Hong Kong (Fig. 5(g)). This results reflects that greenery is better integrated into street space in New York City than in Hong Kong.6.4 Street ExplorerStreet Explorer is developed to enable the efficient comparison of human-scale urban forms at street-scale (T.1.2). The explorer adapts the same layout as that in the AOI Explorer, i.e., juxtaposition map views are placed on the top of the explorer, whereas detailed statistics view are on the bottom.6.4.1 Street Map ViewSimilar to AOI Map View, Street Map View is also developed on top of a background map layer that is implemented using Leaflet.js. Street views on the street are over-viewed as points with colors that correspond to primary features, i.e., green for greenery, light blue for sky and orange for building. In contrast to AOI Map View, Street Map View presents more details of human-scale urban forms by displaying the corresponding street view images along the two sides of a street. The images are evenly selected in the direction of the street heading. In particular, the feature compositions of an image are displayed as a tree map [35] when users hover their mouse pointer over the image. An example is provided in Fig. 6(a).6.4.2 Street Statistic ViewThe design goal for this view is to allow users to quantitatively compare human-scale urban forms from two streets. Although we can utilize the same views shown in the AOI Statistic View, our collaborating domain expert SR is not satisfied. SR strongly recommended encoding street layout information in the view, so that users can better leverage their knowledge about streets to perform in-depth analysis. SR felt the spatial information was not well integrated with the Street Map View. Based on this goal, we experimented with a few alternative designs and informally evaluated design prototypes with SR.  For our first
Fig. 7. Overview of the construction process for Street Statistic View:(a) Construct minimum bounding box of the street network. (b) Rotate the street network such that its bounding box fits in the rendering space.(c) Plot themeriver style visualization within the rendering space. (d) Enhance parallel coordinates with street layouts on both sides.prototype, we designed some glyphs, such as a radial chart or tree map, and directly overlaid the glyphs on the map view. The design was aborted because isolating statistics on the two maps weakened the effects of comparison. In addition, such a design easily generated visual clutter, which requires redundant user interactions or clustering to address. For the second prototype, we encoded street layout information in a similar design as the AOI Statistic View. However, this method was extremely difficult because the rendering space was fully utilized.PCP with Street Layout. Inspired by [32], we ultimately develop a design of a parallel coordinates enhanced with street layouts (Fig. 7(d)). We elaborate the construction of this design as follows:Bounding Box Construction (Fig. 7(a)). The first step is to construct a minimum bounding box (MBB) of the street layout. We find the starting(A) and ending (B) points, and generate a primary axis pointing from A to B. MBB is generated as the minimum rectangle that contains all nodes of the street network parallel with the primary axis.Street Layout Rotation (Fig. 7(b)). Next, we rotate the street layout such that MBB fits in the rendering space, i.e., A and B lay on the top and bottom (or vice versa) sides of the rendering space, and the rotated MBB lays in the center. In the case that MBB is wider than the rendering space, the street layout outside the rendering space is clipped. The rotation direction (clockwise or anticlockwise) is determined based on the direction that produces the minimum rotation angle.ThemeRiver Plotting (Fig. 7(c)). To fully utilize rendering space, we plot a themeriver-style [17] visualization to show changes in feature values along a street layout. Here, the themeriver is plotted in the vertical instead of the traditionally horizontal direction because the street layout is aligned along the y-axis after rotation. Next, given that street views are sampled evenly along the street layout, we map the street views equally onto the y-axis, i.e., y-values of the themeriver plot reflects the relative positions of street views along the street layout.  We start plotting the themeriver visualization using the left side of the rendering space as the baseline and calculate the upper bound x-values for the greenery feature. The upper bound x-values of the greenery layer are then used as baseline values for the next feature, i.e., sky. This process is repeated until all features are plotted. To support better feature comparison, our system allows users to reorder feature sequence by clicking on the feature layer of interest and shifting it to the left side [4].PCP Integration (Fig. 7(d)). The themeriver plot can be used as a coordinate for the PCP. To enable comparison between two streets, we integrate their themeriver plots into a PCP on left and right sides, and the identified features are used as other coordinates. Each street view
Fig. 8. AOI Map View compares spatial distributions of human-scale urban forms in Singapore (left) and Greater London (right). Orange points (buildings) are concentrated around the highlighted center area of Greater London, i.e., City of London.
Greenery
0.5b
0.15
Fig. 10. Top three districts with highest building ratios, while bottom threegreenery
0.00.0
00.5.5
with highest
ratios in Hong Kong.
Sky     a0.00.0Singapore London
00.5.50.00.0Road
00.5.50.00.0Vehicle
00.5.50.00.0	0.5Others
		0.000.15			0.100.05		0.000.15			0.100.05
Others
cific street view on the AOI Map View is selected, the cluster that contains the street view in the scatterplot matrix and the diversity views will be highlighted accordingly.7    CASE STUDIES
0.000.0          0.1          0.2          0.3          0.4          0.5   0.0          0.1          0.2          0.3          0.4          0.5Fig. 9. AOI Statistic View in coordination with Fig. 8 presents quantitative measurements differences of human-scale urban forms in Singapore (red) and Greater London (blue).is conveniently presented as a polygonal line, which is colored as red and blue for left and right street views, respectively. In addition, we aggregate the feature values via binning techniques on left and right sides of each feature coordinate to facilitate the visual comparison of feature distributions.  Fig. 7(d) presents an example Street Statistic View. A first glimpse at the two themeriver plots, we can find that the left street contains more greenery, while on the right street people can see more sky. This can be found in the distribution comparison on the greenery & sky coordinates in the middle. Besides, we can also find that feature values vary dramatically on the left themeriver plot, showing street views are very different along the left street.6.5   User InteractionsIn addition to basic interactions in each view, StreetVizor also provides a control panel (Fig. 1(a)) that enables:• Multi-scale Navigation. To help users navigate effectively across different scales, we develop city-, region- and street-panels. The city-panel lists all four cities, i.e., Hong Kong, Singapore, Greater London, and New York City. Users start navigation by selecting a city. The region-panel will then list all administrative regions under the city. For instance, City of London and Kingston will be listed if Greater London is selected. Similarly, the street-panel lists all the streets inside a selected city or region.• Feature Filtering. Our system also supports filtering human-scale urban forms against a specific feature by specifying the value range with feature sliders. By default, each feature slide within [0- 1], and users can change the minimum and maximum values by dragging the slider buttons.  In addition, our system also supports the following user interactions to facilitate visual exploration.• Details on Demand. To enable overview+details (R.1), we de- velop a set of interactions that allow users to explore the details of human-scale urban forms on demand. For example, if a street view point is selected in the AOI Map View, the correspond- ing image will show up. Thus, users can leverage their domain knowledge by visually examining street views.• Linking. Our system supports automatic linking among the visu- alization modules in both AOI Explorer and Street Explorer for
This section presents three case studies on the application of StreetVizor in assisting urban planners to explore and compare human-scale urban forms on the city-, region-, and street-scales.7.1 City-scale ComparisonComparing Spatial Distribution. With the AOI Map View, our system allows users to compare spatial distributions of human-scale urban forms in two AOIs (T.1.1). Fig. 8 presents a comparison between Singapore (left) and Greater London (right). As shown in the figure, more orange points surround the highlighted white circle, whereas more green points can be found at marginal areas in Greater London. This indicates that more buildings are constructed around the City of London, reflecting that this city is more urbanized compared with other areas. By contrast, Singapore’s urban forms are evenly distributed. More orange points are found in the highlighted downtown and region hubs, and more green points can be found in natural areas. Our collaborator SR explained the reason for the different spatial distributions of urban forms in Greater London and Singapore: Greater London likely expanded its urbanization from the City of London to surrounding areas, whereas Singapore, as an island city, cannot expand.Comparing Quantitative Measurements. We can further explore the differences in quantitative measurements with AOI Statistic View (T.2.1& T.3.3). Fig. 9 shows the AOI Statistic View in coordination with Fig. 8. From the scatterplot matrix, we find that greenery and building features are negatively correlated. This result is the same as that shown in Fig. 5(d). Given that buildings are artifacts and greenery is natural, the negative correlation between building and greenery features in all four cities indicates that artifacts increase and natural environments decrease with increasing urbanization. To improve livability, many urban planners have proposed integrating more natural spaces in street space. In addition, Fig. 9(a) shows that sky and building ratios are higher in London than in Singapore. By contrast, greenery and road ratios are higher in Singapore than in London (Fig. 9(b)). These findings can also be found from the middle histogram bar charts. In addition, the diversity views further show some interesting patterns. For example, building and road diversities are more concentrated in London than those in Singapore, as shown in Fig. 9(c) & (d), and likely resulted from the different standards for building and road construction in the two cities. Vehicle usage rates are low in both cities, as shown in Fig. 9(e).7.2 Region-scale ExplorationGiven that human-scale urban forms are highly associated with the daily lives of residents, we posit that urban forms could reflect the functionalities of a region. Study 1 already reveals the negative correla- tions between greenery and building features, and we hypothesize that these two urban forms can reflect urbanization levels. To evaluate the
															0.00.20.1
0.00.0         0.1         0.2         0.3         0.4         0.5         0.6
0.0         0.1         0.2         0.3         0.4         0.5         0.6
Fig. 11. Region-scale comparison of Tanglin in Singapore with Central Park in New York City.
hypothesis, we further explore these two features at region-scale (T.1.1). Using ranking functions, we sort administrative districts in Hong Kong based on these two features. Fig. 10 presents an overview of three districts with highest values for each feature. The top three districts are Yau Tsim Mong, Wan Chai, and Kowloon City. They are all business centers in Hong Kong, and we find these districts are mostly covered by orange points (building). By contrast, the bottom three districts contain highest values for greenery. In particular, the greenest district is Southern Island, which is reserved as country parks. The other two districts are also well-known natural areas in Hong Kong.  In addition to exploring regions with different functionalities, urban planners are also interested in comparing regions with similar func- tionalities. Here, we leverage our knowledge of famous regions in two different cities, i.e., Tanglin in Singapore (see the region highlighted as natural in Fig. 8) and Central Park in New York City. Both of these regions are well-known parks. After selecting these regions, we find that nearly all streets in the two regions are dominated by green points, indicating that both regions contain considerable greenery as intended. However, we find some differences between these two re- gions by looking deeper into the Street Statistics View (Fig. 11). First, through the feature histogram bar charts, we find that Central Park has slightly higher greenery ratios (A1), whereas Tanglin has slightly higher building ratios (A2). Though these differences are marginal, they reflect that more buildings are present in Tanglin, partially because Singapore tries to maximize land usage for building construction. In addition, by grouping street views based on street units, we glean more information from the diversity plots. Here, we first obtain the overview that greenery feature has relatively higher mean values and deviations than the other five features. We also find some anomalies. For example, some streets have relatively low mean values but high deviations for greenery in Central Park (B1), whereas certain streets have high sky visibility in Tanglin (B2), and some streets have very high building ratios in both regions (B3).  The results of this study show that human-scale urban forms are correlated with regional functionalities. Meanwhile, even though two regions may have the same functionalities, street spaces in regions can be different between two cities. This may reflect the differences in city planning and development strategies between cities.7.3 Street-scale ComparisonOur collaborating domain expert SR is interested in comparing the fine- grained details of two streets (T.1.2). StreetVizor meets this require- ment with Street Explorer. Here, we select one street from Brooklyn, New York City, and one from Kowloon, Hong Kong. Both streets are representative streets of each district.  Fig. 12 presents the visual comparison of these two streets. The map views provide an overview that nearly half of the street views are mostly green (greenery), and the other half are mostly orange (building) on the street from Brooklyn. By contrast, the street views in Kowloon are mostly orange (building). We can observe more details by looking at the street view images. As shown on the top images, more greenery and sky with low buildings are seen in the left two
Fig. 12. Street Explorer compares the differences in human-scale urban forms of two streets in Brooklyn, New York City (left) and Kowloon, Hong Kong (right). The left street views contain more balanced features, whereas the right street views are dominated by building and road.images, whereas the right two images are filled with building and road. The tree map of the leftmost image shows the street view is well balanced with greenery, road, building, and vehicle features. Notice that the street view is also highlighted in the bottom Street Statistic View. More details on the quantitative measurements of street views can be observed in the bottom Street Statistic View. The themeriver plots clearly show differences in feature distributions: street views in Brooklyn are more mixed with balanced greenery, sky, building, road, and vehicle features, whereas street views in Kowloon are mostly filled with building and road features. The histogram bars in PCP further confirm this observation: most street views in Kowloon have less than 10% greenery and sky.  Though it is well known in the field of urban planning that New York City is well integrated with natural features and that Hong Kong has more high-rise buildings, SR is excited to see our system can present these differences so intuitively. “Street Explorer can definitely improve our work efficiency,” SR commented.8 EXPERT  REVIEWTo evaluate the effectiveness of StreetVizor, we conducted expert inter- views with two independent domain experts other than our collaborating senior researcher SR. One of them focuses on designing livable public space (denoted as EA), and the other is an urban ecologist aiming at improving greenery in cities (denoted as EB). Hence, the experts are from different backgrounds: SR and EA in urban planning, while EB in ecology. In the interviews, we started with explaining the visual encodings and interface design, and demonstrated to them how our sys- tem works. Then, we showed them the case studies, and allowed them to explore the system by themselves for about twenty minutes in the end. In general, both experts agrees that the way we study human-scale urban forms with street view images is a promising direction. Their detailed feedbacks are summarized below.Methodology and Approach. EA agreed with SR that evidence-based urban design is becoming a trend in urban planning. He commented, “StreetVizor is far more than simply a visualization platform. Rather, it is an excellent combination of machine learning and visualization tech- niques, together with classical urban design theories in place-making”. EB also expressed that “street view images as an emerging data source can reflect urban environments well”, and a visual analytics system can greatly facilitate the exploration of street views.Interactive Visual Design. Both experts confirmed that StreetVizor is nicely designed according to the problem domain. They appreciated the visual consistency across different views. EA highlighted “it is very

important to use the same colors in different views”.  SR agreed the workflow of ranking multiple AOIs/streets with Ranking Explorer, and comparing two AOIs/streets for details through AOI and Street Explorer is helpful. “It is easier for me to identify interested regions, such as those in Hong Kong (Fig. 10)”, commented by SR. The AOI Explorer is nicely designed with intuitive map and statistic views. In particular, “the statistic view seamlessly integrates three easily understandable plots”, commented by EB. By referring to the scatterplot matrix in Fig. 5 & Fig. 9, EA was excited to see the negative correlation betweengreenery and building − “there are much space to improve”. EA also liked the visual comparison of human-scale urban form distributionsover space (Fig. 8), as it reflects the differences of urbanization process and master plans in different cities.  The experts thought presenting street view images in the Street Ex- plorer is intuitive, and mouse hover over to show tree map is helpful. In contrary, it was difficult in the beginning for both experts to understand the PCP enhanced with street layout information, especially the the- meriver plot. But after exploration, they agreed it is an excellent idea, as it clearly reveals the feature distributions along street. “Though not common, I believe there are many applications and potentials for the enhanced PCP”, commented by EB.Applicability. Both domain experts would like to apply our system to deal with practical problems in their domains. Expertized in urban planning and design, EA emphasized “the lack of efficient tools for human-scale management and design obstacles creating high-quality urban streets.” StreetVizor has a great potential to be employed by planners and designers to “build more pedestrian-oriented and livable streets.” EA also commented “StreetVizor is highly applicable for evaluating case studies in urban planning”. For example, planners can select several key areas, e.g., CBDs, among different cities and then compare their spatial features to develop appropriate strategies in urban renewal. EB would like to apply our system in environment monitoring, since “the large amount of GSV images can provide rich information of urban environment”. He suggested to extend our system in exploring a time-series street view dataset, so that it would allow him to monitor environment changes.Limitations and Improvements. The experts pointed out some limi- tations in our system. In this work, we explore only six features of human-scale urban forms. Both experts suggested to extract more urban forms, such as aesthetic amenities and mental well-being, from street view images. This will advance our system’s analytical capabilities and extend its applicabilities. For instance, a recent study [21] shows that certain relationship may exist between street greenery & sky ratio and the risk of health challenges. Besides, they proposed to improve our visual designs. EB noticed the feature histogram bars in AOI and Street Statistic View are designed differently: one in horizontal, and the other one in vertical style. He felt the vertical histogram bars are confusing, as they “do not fit our work habits”. EA suggested the Street Statistic View can be further improved, by “encoding neighboring street layouts in the plot”, to reflect spatial information better.9 DISCUSSIONIn this work, we combine automatic machine learning and interactive visual analytics techniques to explore human-scale urban forms. The combination of methods tackles the challenges of integrating informa- tion from multiple perspectives and at different scales for analysis. This approach is attractive for urban planners [25, 26] because it shows the possibility of transferring traditional subjective and intuitive-oriented urban design to evidence-based and big-data informed methods.  The analysis of human-scale urban form in this work relies heavily on a deep learning technique for image classification. Therefore, classi- fication accuracy poses serious challenges in our approach. We select SegNet, which achieves a global accuracy of 82.8%, out of the different classification techniques that we tried. The case studies show that the classification technique could provide reasonable analytical results on city- and region-scale human-scale urban form patterns. Nonetheless, the results remain unsatisfactory in many cases, especially when users would like to examine fine detailed urban forms at street-scale.  We
envision applying a more advanced classification algorithm in the near future, given the rapid evolution of image classification techniques. In addition, the classification is preprocessed offline. Our system does not support the analysis of street views queried on runtime. Thus, its applicability is limited and domain knowledge of planners are underuti- lized in exploring street views in other cities. This deficiency can also be resolved with advancement in image classification algorithms and machine computing capabilities.  Moreover, we expect that our system will face scalability issues when the number of street view images increase (e.g. when analyzing street views in more cities). To tackle this problem, we can integrate more advanced data structures, such as nanocubes [23] or Gaussian cubes [40] for spatial data querying. More levels of detail and abstrac- tion can also be introduced to handle this problem. The increase in image number will also burden street view clustering in the interactive exploration process. We anticipate that certain pre-configurations will facilitate this process.  Presenting multivariate data with spatial information is challenging. We tackle this challenge by integrating popular PCP with a themeriver plot along an adjusted street layout. The case studies and feedbacks from experts demonstrate the effectiveness of this design. Nonetheless, there are some issues with our design. First, it represents street layout as a simple spline, which is not sufficiently intuitive when the street is straight. We plan to encode more semantic labels, such as neigh- boring streets, in the design, as suggested by EA. Second, although the majority of the streets (over 95%) that we have explored can be rotated and fitted in the rendering space, adjustment does not work in some cases. Typical examples are streets in a spiral layout. A more general approach should be developed that can reveal the street layout intuitively and seamlessly fit the street layout in the rendering space.10 CONCLUSION AND FUTURE WORKIn this paper, we introduce StreetVizor, a visual analytics system for the exploration of human-scale urban forms based on GSV images. Through discussions with a collaborating researcher who specializes in evidence-based urban planning, we identify various analysis criteria and formulate a set of analytical tasks. We integrate some well-estimated visualization techniques, such as juxtaposition map views, scatterplot matrix, and small multiples, into our system. Specifically, we design an enhanced parallel coordinates with street layout to present coor- dinated feature values and reveal feature distributions along a streetlayout. StreetVizor is used to analyze ∼1.7 million street views from Hong Kong, Singapore, Greater London, and New York City. Usingour system, domain experts detect some interesting patterns, such as the negative correlation between greenery and building features. The experts also agree that our system has a wide range of applications, in areas like public space design and street environment monitoring.  There are several promising directions for future work. First, we would like to extract more high-level information, such as sign- boards, from GSV images. The signboards can then be compared with points-of-interest information extracted from other data sources, e.g., Foursquare or Google Places. We anticipate revealing some interest- ing patterns by fusing multiple types of big urban data. In addition, to address the scalability issues, we plan to develop more advanced data structures (e.g., [23, 40]) to improve the querying and filtering processes in our method. Besides, we would like to explore new visual interfaces to compare detailed statistics of human-scale urban forms across multiple AOIs/streets at the same time.ACKNOWLEDGMENTSThe authors wish to thank anonymous reviewers for their constructive comments. The research was established at the Future Cities Laboratory at the Singapore-ETH Centre, which was established collaboratively be- tween ETH Zurich and Singapore’s National Research Foundation (FI 370074016) under its Campus for Research Excellence and Technolog- ical Enterprise programme. The work was supported in part by a grant from Urban China Initiative 2017, HK RGC GRF 16241916, National 973 Program of China (2014CB340304), and ITF ITS/170/15FP.

REFERENCES[1]  D. Anguelov, C. Dulong, D. Filip, C. Frueh, S. Lafon, R. Lyon, A. Ogale,L. Vincent, and J. Weaver.  Google street view: Capturing the world at street level. Computer, 43(6):32–38, 2010.[2] S. M. Arietta, A. A. Efros, R. Ramamoorthi, and M. Agrawala. City Forensics: Using visual elements to predict non-visual city. IEEE Trans. Vis. Comput. Graph., 20(12):2624 – 2633, 2014.[3] V. Badrinarayanan, A. Kendall, and R. Cipolla. SegNet:  A deep con- volutional encoder-decoder architecture for image segmentation. arXiv preprint, abs/1511.00561, 2015.[4]  L. Byron and M. Wattenberg. Stacked graphs – geometry & aesthetics.IEEE Trans. Vis. Comput. Graph., 14(6):1245 – 1252, 2008.[5] H. Chen, W. Chen, H. Mei, Z. Liu, K. Zhou, W. Chen, W. Gu, and K.-L. Ma. Visual abstraction and exploration of multi-class scatterplots. IEEE Trans. Vis. Comput. Graph., 20(12):1683–1692, 2014.[6] S. Chen, X. Yuan, Z. Wang, C. Guo, J. Liang, Z. Wang, X. Zhang, and J. Zhang. Interactive visual discovering of movement patterns from sparsely sampled geo-tagged social media data. IEEE Trans. Vis. Comput. Graph., 22(1):270 – 279, 2016.[7] D. Comaniciu and P. Meer. Mean shift: A robust approach toward feature space analysis. IEEE Trans. Pattern Anal. Mach. Intell., 24(5):603–619, 2002.[8] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes Paris look like Paris? Commun. ACM, 58(12):103–110, 2015.[9] N. Ferreira, L. Lins, D. Fink, S. Kelling, C. Wood, J. Freire, and C. T. Silva. BirdVis: Visualizing and understanding bird populations. IEEE Trans. Vis. Comput. Graph., 17(12):2374–2383, 2011.[10] N. Ferreira, J. Poco, H. T. Vo, J. Freire, and C. T. Silva. Visual exploration of big spatio-temporal urban data: A study of New York city taxi trips. IEEE Trans. Vis. Comput. Graph., 19(12):2149–2158, 2013.[11] Y.-H. Fua, M. O. Ward, and E. A. Rundensteiner. Hierarchical parallel coordinates for exploration of large datasets. In Proc. IEEE Conf. Vis., pages 43–50, 1999.[12] T. Gebru, J. Krause, Y. Wang, D. Chen, J. Deng, E. L. Aiden, and F.-F. Li. Using deep learning and Google street view to estimate the demographic makeup of the US. arXiv preprint arXiv:1702.06683, 2017.[13] J. Gehl. Life between Buildings: Using Public Space. Van Nostrand Reinhold, 1971.[14] M. Gleicher, D. Albers, R. Walker, I. Jusufi, C. D. Hansen, and J. C. Roberts. Visual comparison for information visualization. Inf. Vis., 10(4):289–309, 2011.[15] S. Goodwin, J. Dykes, A. Slingsby, and C. Turkay. Visualizing multiple variables across scale and geography. IEEE Trans. Vis. Comput. Graph., 22(1):599–608, 2016.[16]  Google.	Street	View	Image	API. https://developers.google.com/maps/documentation/streetview/. Accessed: 2017-03-20.[17] S. Havre, E. Hetzler, P. Whitney, and L. Nowell. ThemeRiver: Visualizing thematic changes in large document collections. IEEE Trans. Vis. Comput. Graph., 8(1):9–20, 2002.[18]  J. Heinrich and D. Weiskopf. State of the art of parallel coordinates. InEurographics - State of The Art Report, pages 95 – 116, 2013.[19] D. Holten and J. J. van Wijk. Evaluation of cluster identification perfor- mance for different PCP variants. Comput. Graph. Forum, 29(3):793–802, 2010.[20] J. Jacobs. The Life and Death of Great American Cities. Random House, New York, 1961.[21] B. Jiang, C.-Y. Chang, and W. C. Sullivan. A dose of nature: Tree cover, stress reduction, and gender differences. Landsc. Urban Plan., 132:26–36, 2014.[22] X. Li, C. Zhang, W. Li, R. Ricard, Q. Meng, and W. Zhang. Assessing street-level urban greenery using Google street view and a modified green view index. Urban For. Urban Gree., 14(3):675–685, 2015.[23] L. Lins, J. T. Klosowski, and C. Scheidegger. Nanocubes for real-time exploration of spatiotemporal datasets. IEEE Trans. Vis. Comput. Graph., 19(12):2456–2465, 2013.[24] D. Liu, D. Weng, Y. Li, J. Bao, Y. Zheng, H. Qu, and Y. Wu. SmartAdP: Visual analytics of large-scale taxi trajectories for selecting billboard locations. IEEE Trans. Vis. Comput. Graph., 23(1):1–10, 2017.[25] X. Liu, Y. Song, K. Wu, J. Wang, D. Li, and Y. Long. Understanding urban China with open data. Cities, 47:53 – 61, 2015.[26]  Y. Long and L. Liu.  How green are the streets? an analysis for central
areas of Chinese cities using Tencent street view. PLOS ONE, 12(2):1–18, 02 2017.[27] Y. Long and Y. Ye. Human-scale urban form: Measurements, perfor- mances, and urban planning & design interventions. South Architecture, 36(5):39 – 45, 2016.[28] N. Naik, J. Philipoom, R. Raskar, and C. Hidalgo. Streetscore – predicting the perceived safety of one million streetscapes. In Proc. IEEE Conf. Comput. Vision and Pattern Recognition Workshops, pages 793–799, 2014.[29] OpenStreetMap. OSM Wiki. http://wiki.openstreetmap.org/wiki/Main Page.Accessed: 2017-03-20.[30]  T.  Ortner,  J.  Sorger,  H.  Steinlechner,  G.  Hesina,  H.  Piringer,  andE. Groeller. Vis-A-Ware: Integrating spatial and non-spatial visualization for visibility-aware urban planning. IEEE Trans. Vis. Comput. Graph., 23(2):1139 – 1151, 2016.[31] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel,M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas- sos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit- learn: Machine learning in Python. J. Mach. Learn. Res., 12:2825–2830, 2011.[32] H. Qu, W.-Y. Chan, A. Xu, K.-L. Chung, K.-H. Lau, and P. Guo. Visual analysis of the air pollution problem in Hong Kong. IEEE Trans. Vis. Comput. Graph., 13(6):1408–1415, 2007.[33] A. G. Rundle, M. D. Bader, C. A. Richards, K. M. Neckerman, and J. O. Teitler. Using Google street view to audit neighborhood environments. Am. J. Prev. Med., 40(1):94–100, 2011.[34] R. Scheepens, N. Willems, H. van de Wetering, G. Andrienko, N. An- drienko, and J. J. van Wijk. Composite density maps for multivariate trajectories. IEEE Trans. Vis. Comput. Graph., 17(12):2518–2527, 2011.[35] B. Shneiderman. Tree visualization with Tree-maps: 2-d space-filling approach. ACM Trans. Graph., 11(1):92–99, 1992.[36] B. Shneiderman. The eyes have it: A task by data type taxonomy for information visualizations. In Proc. IEEE Symp. Visual Languages, pages 336–343, 1996.[37] G. Sun, Y. Wu, R. Liang, and S. Liu. A survey of visual analytics tech- niques and applications: State-of-the-art research and future challenges. J. Comput. Sci. Technol., 28(5):852–867, 2013.[38] C. Turkay, A. Slingsby, H. Hauser, J. Wood, and J. Dykes. Attribute signa- tures: Dynamic visual summaries for analyzing multivariate geographical data. IEEE Trans. Vis. Comput. Graph., 20(12):2033–2042, 2014.[39] C. A. Vanegas, D. G. Aliaga, B. Benes, and P. Waddell.  Visualization of simulated urban spaces: Inferring parameterized generation of streets, parcels, and aerial imagery. IEEE Trans. Vis. Comput. Graph., 15(3):424– 435, 2009.[40] Z. Wang, N. Ferreira, Y. Wei, A. S. Bhaskar, and C. Scheidegger. Gaussian cubes: Real-time modeling for visual exploration of large multidimen- sional datasets. IEEE Trans. Vis. Comput. Graph., 23(1):671 – 680, 2017.[41] Z. Wang, M. Lu, X. Yuan, J. Zhang, and H. van de Wetering. Visual traffic jam analysis based on trajectory data. IEEE Trans. Vis. Comput. Graph., 19(12):2159–2168, 2013.[42] P. Xu, Y. Wu, E. Wei, T.-Q. Peng, S. Liu, J. J. H. Zhu, and H. Qu. Visual analysis of topic competition on social media. IEEE Trans. Vis. Comput. Graph., 19(12):2012–2021, 2013.[43] X. Yang, L. Shi, M. Daianu, H. Tong, Q. Liu, and P. Thompson. Blockwise human brain network visual comparison using nodetrix representation. IEEE Trans. Vis. Comput. Graph., 23(1):181 – 190, 2017.[44] W. Zeng, C.-W. Fu, S. Mu¨ ller Arisona, S. Schubiger, R. Burkhard, and K.-L. Ma. Visualizing the relationship between human mobility and points-of-interest. IEEE Trans. Intell. Transp. Syst., 2017.[45] X. Zhao and A. Kaufman. Structure revealing techniques based on parallel coordinates plot. Vis. Comput., 28(6):541–551, 2012.[46] Y. Zheng, W. Wu, Y. Chen, H. Qu, and L. M. Ni. Visual analytics in urban computing: An overview. IEEE Trans. Big Data, 2(3):276–296, 2016.[47] H. Zhou, X. Yuan, H. Qu, W. Cui, and B. Chen. Visual clustering in parallel coordinates. In Comput. Graph. Forum, volume 27, pages 1047–1054, 2008.